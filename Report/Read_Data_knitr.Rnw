\documentclass[11pt, a4paper]{article} %or article has only section and below, book and report also have chapter: http://texblog.org/2007/07/09/documentclassbook-report-article-or-letter/

\usepackage[utf8]{inputenc}  % use utf8 encoding of symbols such as umlaute for maximal compatibility across platforms

\usepackage{caption}				% provides commands for handling caption sizes etc.
\usepackage[a4paper, left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}		 % to easily change margin widths: https://www.sharelatex.com/learn/Page_size_and_margins



\usepackage{etoolbox}    % for conditional evaluations!
\usepackage[bottom]{footmisc}  % I love footnotes! And they should be down at the bottom of the page!
\usepackage{graphicx}        % when using figures and alike
\usepackage[hidelinks]{hyperref}		% for hyperreferences (links within the document: references, figures, tables, citations)

\usepackage{euler}     % a math font, only for equations and alike; call BEFORE changing the main font; alternatives: mathptmx, fourier, 
%\usepackage{gentium} % for a different font; you can also try: cantarell, charter, libertine, gentium, bera, ... http://tex.stackexchange.com/questions/59403/what-font-packages-are-installed-in-tex-live

%\usepackage{listings}
%\lstset{breaklines=T}

%------------------------------------------------------------------------------------------------------
%------- text size settings --------------
\setlength{\textwidth}{16cm}% 
\setlength{\textheight}{25cm} %23 
%(these values were used to fill the page more fully and thus reduce the number of pages!)
\setlength{\topmargin}{-1.5cm} %-1.5
\setlength{\footskip}{1cm} %
%\setlength{\hoffset}{0cm} %
\setlength{\oddsidemargin}{0cm}%1
\setlength{\evensidemargin}{0cm} %-0.5
\setlength{\parskip}{0cm} % Abstand zwischen Abs√§tzen
% ----------------------------------------------------------------
\renewcommand{\textfraction}{0.1} % allows more space to graphics in float
\renewcommand{\topfraction}{0.85}
%\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.70}


\frenchspacing %http://texwelt.de/wissen/fragen/1154/was-ist-french-spacing-was-macht-frenchspacing

\sloppy

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\begin{document}
%\SweaveOpts{concordance=TRUE}
%%%%%%%%%%%%% this bit is new to Knitr: %%%%%%%%%%%%%%%%%%%%%
<<setup, include=FALSE, cache=FALSE, echo=F>>=
library(knitr)
# library(formatR)
# set global chunk options
opts_chunk$set(fig.path='', fig.align='center', fig.show='hold', tidy=FALSE)
options(replace.assign=TRUE, width=80)
#render_listings()
@


\title{A tutorial for Step Selection Function}

\author{P. Antkowiak\thanks{M.Sc. programme "GIS und Umweltmodellierung" at University of Freiburg} \and H. Tripke\thanks{M.Sc. programme "Wildlife, Biodiversity and Vegetation" at University of Freiburg} \and C. Wilhelm\thanks{M.Sc. programme "Wildlife, Biodiversity and Vegetation" at University of Freiburg}}
% for more control, multiple affiliations, line breaks and alike, use the authblk package!!

\date{\today} % !!use package isodate for more control of date formatting!!

\maketitle

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\tableofcontents

\newpage

\section{Introduction}%------------------------------------------------------------------------------------------------------
In addition to Resources Selection Functions (RSF) a more detailed anlysis for telemetry data can be conducted by using Step Selection Function (SSF). The use of latter is providing answers to th actual selection of animals on their habitat rather than analysing the use of a habitat only. (DO WE WANT TO CITE, HERE FOR EXAMPLE Simone and friends). So far most of the SSF were done in GIS to use spatial data together with mathematics equations. However, more and more packages are provided in R and thus becomes a valuable alternative. \
This tutorial provides an overview on how to implement SSF with R. The main package we will use in the following is the package \textbf{adehabitatLT}. All single steps that need to be taken care of are summarized in (Figure~\ref{fig:Flowchart}).\
In our tutorial we use telemetry data from Cougars/Mountain Lion (\textit{Latin name}) collected by Simone Ciuti\footnote{we might have to specify that and name and thank the institute.../collected/containing}. As spatial parameters x tables for ruggedness, slope, canopy cover etc. are available. Describe study area...\

\[
\displaystyle w(x) = exp(\beta\1 x1 + \beta\2 x\2 + ... + \beta\px\p)
\]
\
Why did we not use the data (Wildboar) prepared for the adehabitatLT package? - No information on details, metadata provided, it is hard to understand when to use which dataset and why.


\begin{figure} % you can (but shouldn't) use [h] behind {figure} to force the picture to go here. However, the idea of LaTeX is that it will do things for you, so too much interfering is not saving you any time.
% see also here: http://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions#Captions
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{Flowchart.pdf} %our perfect workflow!
\caption{\emph{Conducting a SSF using existing R-packages:} this figures provides an overview of the steps necessary to conduct a SSF. The steps are separated in subsections, which the turtorial will guide you through.}
\label{fig:Flowchart}
\end{figure}


\section{Preparations}
Before you can actually start using the tutorial for conducting SSF you need to load a bunch of packages in R. Some of them require others so that you have to add all these to your library:

\subsection{Packages - what we need}

<<eval=FALSE>>=
# installing packages -----------------------------------------------------
## for implementing SSF
# install.packages("adehabitat") # outdated version, not needed for this tutorial
install.packages("adehabitatHR")
install.packages("adehabitatHS")
install.packages("adehabitatLT")
install.packages("adehabitatMA")
install.packages("tkrplot")
install.packages("hab", repos = "http://ase-research.org/R/") # regular
install.packages("hab", repos = "http://ase-research.org/R/", type = "source") # for self-compiling

# for handling ratser data
install.packages("move")
install.packages("raster")
install.packages("rgdal")
#install.packages("")

# loading the packages
# require(adehabitat) # keep fingers off this package. It is outdated.
require(hab)
require(adehabitatMA)
require(adehabitatHR)
require(adehabitatHS)
require(adehabitatLT)


## for i dont know

#require(move)
#require(raster)
#require(rgdal)
#require(tkrplot)
#require(raster)
#require(sp)

@



\section{Loading telemetry data (*.csv, ESRI)}%------------------------------------------------------------------------------------------------------

The data for the analysis should be saved in a simple *.csv file format. The table should have column headings in the first line and for each observation include at least the following values: 
\begin{enumerate}
\item{x-coordinate (easting)}
\item{y-coordinate (northing)}
\item{date and/or time}
\item{animal ID}
\end{enumerate}

\noindent Note that the coordinates need to be provided in the same coordinate system and spatial projection as the raster data.

Depending on your analysis you can include further values such as:
\begin{enumerate}
\item{ID for each record}
\item{GPS precision}
\item{other recording parameters such as season}
\item{temperature / elevation at the moment of record}
\item{other values that might be of interest in the further analysis}
\end{enumerate}


\noindent Use the following commands to set your working directory and read the data:
<<eval=FALSE>>=
setwd("/your/working/directory")
xmpl = read.csv("xmpl.csv", head=T)
@

\noindent You can execute \texttt{head(xmpl)} and \texttt{str(xmpl)} to check, whether the data were successfully read.



\section{Creating a Spatial Points Data Frame}%------------------------------------------------------------------------------------------------------
The functions used along the rest of the toolchain can only process data that are stored as objects of class "SpatialPointsDataFrame". This object class stores the coordinates separately and can be created using the according function from package "sp". 

<<eval=FALSE>>=
require(sp)

xmpl.spdf = SpatialPointsDataFrame(coords = xmpl[,c("easting","northing")], data = xmpl)

names(xmpl)
@
In the "coords" argument of the function you should specify the coordinate vectors for your dataset. In our example, we simply assign the two coordinate columns of the example dataset, but you can read the coordinates from a separate file if you want.


\section{Creating an ltraj object}%------------------------------------------------------------------------------------------------------
After storing the data in a Spatial Points Data Frame, you now need to connect the single points and turn them into a set of trajectories. This operation is carried out by the function \texttt{as.ltraj} from the "hab" package and produces objects of class "ltraj".
The function \texttt{as.ltraj} requires at least three arguments to work:
\begin{enumerate}
\item{"xy" (x- and y- coordinates for each point)}
\item{"date" (timestamp for each point, given as POSIXct class)}
\item{"id" (the animal id)}
\end{enumerate}

Both coordinates and animal id can easily be adopted from the Spatial Points Data Frame. The timestamp however, is not stored in the required format yet and you therefore need to convert it first:
<<eval=FALSE>>=
date <- as.POSIXct(strptime(paste(xmpl.spdf$LMT_DATE, xmpl.spdf$LMT_TIME), "%d/%m/%Y  %H:%M:%S"))
@
It is necessary to combine date and time in one POSIXct value. If your dataset already features a POSIXct timestamp, you can skip this step.

Now you can proceed and actually create the ltraj object by executing the following command:
<<eval=FALSE>>=
xmpl.ltr <- hab:::as.ltraj(xy = xmpl.spdf@coords, date = date, id = xmpl.spdf$cat) 
@
Two comments to the function as used: By typing \texttt{hab:::as.ltraj} you tell R to use the \texttt{as.ltraj} function from the "hab" package which is speed optimized against its \texttt{adehabitatLT} sibling. Unlike the "xmpl.spdf@coords" promt which works for any SPDF object, the \texttt{xmpl.spdf\$cat} prompt is specific to your dataset. In the example dataset, animal ID's are stored as an integer vector called "cat". If this differs in your dataset and you should change the prompt accordingly.

You now may want to have a closer look at the created ltraj object. Display its structure with by executing \texttt{str(xmpl.ltr)}. 
<<eval=FALSE>>=
str(xmpl.ltr)
@
When scrolling through the output you will first notice that it consists of 7 elements - the number of individuals in the example dataset. This is because \texttt{as.ltraj()} automatically splits the dataset into subsets one for each individual. We will refine these subsets in the next chapter. Furthermore, the ltraj contains information on the distances and turning angles between consecutive locations.
To get a visual impression of your data you can plot the trajectory for all or for one particular animal:
<<eval=FALSE>>=
plot(xmpl.ltr)
unique(xmpl.spdf$cat) # prompts a list of all cat ID's. Choose one that you are interested in!
plot(xmpl.ltr, id=10289)
@

\section{Creating bursts}


\section{Compute random steps}

The function \textbf{rdSteps} removes the first and the last data point. That's what you want. 



\section{Spatial covariates}%----------------------------------------------------------------------------------------------------------------------------
This section explains the handling of spatial parameters that will be tested for selection by the target species. You should store these data in raster files (ESRI *.adf or georeferenced *.tif). These should have the same coordinate system as your telemetry data and should (for time reasons) already be clipped to your study area and. For instructions how to do this in R, please read the GIS instructions from the other group ;)   

\subsection{Load raster data (ESRI, *.tif, (*.shp))}%------------------------------------------------------------------------------------------------------

With a simple function stored in the package \textbf{raster} you are able to upload any raster file into R. Examplarily we use raster data on the following parameters for the study area:
\begin{enumerate}
\item{ruggedness of the terrain}
\item{land cover}
\item{canopy cover}
\item{distance to the nearest highway}
\item{distance to the nearest road}
\end{enumerate}

For reading the raster data, three packages are required:
<<eval=FALSE>>=
require(raster)
require(rgdal)
require(sp)
@

The source files for the raster data can be stored in the working directory or loaded by specifying the exact path. The \texttt{raster()} function is a universal and very powerful tool for loading all kinds of raster data. For reading shapefiles, use the \texttt{readOGR()} function. Below is an example of how to read a set of raster layers.
<<eval=FALSE>>=
# First, make sure that your working directory is still the one specified earlier:
getwd()

# Now read the layers:
ruggedness <- raster("ruggedness.adf") 
landcover <- raster("landcover.adf") 
canopycover <- raster("canopycover.adf") 
disthighway <- raster("disthighway.adf")
distroad <- raster("distroad.adf")
# In the ESRI directory system, raster layers are usually stored in files called "w001001.adf". For demonstration purpose, we copied and renamed them.
@

You can plot the data for a first overview. As this can take a while with large datasets, outcomment the following chunk if you please!
<<eval=FALSE>>=
plot(ruggedness) 
plot(landcover)
plot(canopycover)
plot(disthighway)
plot(distroad)
@

\subsection{Raster extraction}
Now that you have generated the random steps and loaded the raster data, you can take the next step and actually connect the trajectories with the spatial covariates. There are different functions that can do this. When choosing one, you need to consider that raster files are large and juggling with them occupies lots of memory and computing power. For this reason we suggest using the \texttt{extract()} function that allows for querying single pixel values without loading the whole source file into working memory. The code for compiling the final dataset involves three steps: Converting the \texttt{xmpl.steps} data frame into a Spatial Points Data frame, extracting the raster values and combining them to the final dataset.
Converting \texttt{xmpl.steps} into a SpatialPointsDataFrame:
<<eval=FALSE>>= 
xmpl.steps.spdf <- SpatialPointsDataFrame(coords = xmpl.steps[,c("new_x","new_y")], data = xmpl.steps)
@

Extracting the values from each raster layer:
<<eval=FALSE>>= 
ruggedness.extr <- extract(ruggedness, xmpl.steps.spdf, method='simple', sp=F, df=T) 
canopycover.extr <- extract(canopycover, xmpl.steps.spdf, method='simple', sp=F, df=T)
disthighway.extr <- extract(disthighway, xmpl.steps.spdf, method='simple', sp=F, df=T)
distroad.extr <- extract(distroad, xmpl.steps.spdf, method='simple', sp=F, df=T)
landcover.extr <- extract(landcover, xmpl.steps.spdf, method='simple', sp=F, df=T)
@
The extraction is done separately for each layer. The option \texttt{method = 'simple'} extracts value from nearest cell whereas \texttt{method = 'bilinear'} interpolates from the four nearest cells. You can adjust this option according to the resolution of your dataset and ecological considerations. df=T returns the result as a data frame and sp=F ensures that the output is not added to the original dataset right away. 

Automatically adding the extracted values to the original dataset sounds like a handy option. For two reasons we do not use it here: Firstly, we want to set the column names manually for not ending up with several columns called "w001001". Secondly, our data include a categorial covariate (landcover) that we want to reclassify and flag as a factor.

This is the code for compiling the final dataset:
<<eval=FALSE>>= 
xmpl.steps.spdf$ruggedness <- ruggedness.extr[,2]
xmpl.steps.spdf$canopycover <- canopycover.extr[,2]
xmpl.steps.spdf$disthighway <- disthighway.extr[,2]
xmpl.steps.spdf$distroad <- distroad.extr[,2]

# The landcover covariate comes coded in integers between 0 an 10 and is by default (mis)interpreted as an integer string. 
unique(landcover.extr[,2])
# Re-classifying landcover:
xmpl.steps.spdf$landcover <- as.factor(
    ifelse(landcover.extr[,2] == 0,"NA",
    ifelse(landcover.extr[,2] < 5, "forest", 
    ifelse(landcover.extr[,2] < 8, "open","NA"))))   
@
Now your final dataset should be ready for analysis. Examine it:  
<<eval=FALSE>>=
head(xmpl.steps.spdf)
@




\subsection{Extract coordinates for comparison of used and random points} %------------------------------------------------------------------------------------------------------
Peter is successfully doing this step!!

\section{Final SSF model}


\section{Acknowledgements}
Don't forget to thank TeX and R and other opensource communities if you use their products! The correct way to cite R is shown when typing ``\texttt{citation()}'', and ``\texttt{citation("mgcv")}'' for packages.

\section{Appendix / Literature}

Session Info:
<<echo=FALSE>>=
sessionInfo()
@

\end{document}